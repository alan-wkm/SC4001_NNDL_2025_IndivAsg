{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnRX6LUnqBpw"
   },
   "source": [
    "CS4001/4042 Assignment 1\n",
    "---\n",
    "Part B, Q1 (15 marks)\n",
    "---\n",
    "\n",
    "Real world datasets often have a mix of numeric and categorical features – this dataset is one example. To build models on such data, categorical features have to be encoded or embedded.\n",
    "\n",
    "PyTorch Tabular is a library that makes it very convenient to build neural networks for tabular data. It is built on top of PyTorch Lightning, which abstracts away boilerplate model training code and makes it easy to integrate other tools, e.g. TensorBoard for experiment tracking.\n",
    "\n",
    "For questions B1 and B2, the following features should be used:   \n",
    "- **Numeric / Continuous** features: dist_to_nearest_stn, dist_to_dhoby, degree_centrality, eigenvector_centrality, remaining_lease_years, floor_area_sqm\n",
    "- **Categorical** features: month, town, flat_model_type, storey_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jA67PbIY3PnH"
   },
   "outputs": [],
   "source": [
    "# !pip install pytorch_tabular[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Jr6P3U7w3NVl"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "import os\n",
    "\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.models import CategoryEmbeddingModelConfig\n",
    "from pytorch_tabular.config import (\n",
    "    DataConfig,\n",
    "    OptimizerConfig,\n",
    "    TrainerConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding this to deal with the following warnings\n",
    "# FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through\n",
    "# chained assignment using an inplace method. The behavior will change in pandas 3.0.\n",
    "# This inplace method will never work because the intermediate object on which we are\n",
    "# setting values always behaves as a copy. For example, when doing\n",
    "# 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)'\n",
    "# or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGyEWcVlqKTz"
   },
   "source": [
    "> Divide the dataset (‘hdb_price_prediction.csv’) into train and test sets by using entries from year 2020 and before as training data, and year 2021 as test data (validation set is not required).\n",
    "**Do not** use data from year 2022 and year 2023.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hoCPcOWupw5Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['month', 'year', 'town', 'full_address', 'nearest_stn',\n",
      "       'dist_to_nearest_stn', 'dist_to_dhoby', 'degree_centrality',\n",
      "       'eigenvector_centrality', 'flat_model_type', 'remaining_lease_years',\n",
      "       'floor_area_sqm', 'storey_range', 'resale_price'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('hdb_price_prediction.csv')\n",
    "\n",
    "# TODO: Enter your code here\n",
    "# Filter out rows for years 2022 and 2023\n",
    "filtered_data = df[df['year'] <= 2021]\n",
    "\n",
    "# Split into training and test sets\n",
    "train_data = filtered_data[filtered_data['year'] <= 2020]\n",
    "test_data = filtered_data[filtered_data['year'] == 2021]\n",
    "print(train_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sebMgSuzqPe7"
   },
   "source": [
    "> Refer to the documentation of **PyTorch Tabular** and perform the following tasks: https://pytorch-tabular.readthedocs.io/en/latest/#usage\n",
    "- Use **[DataConfig](https://pytorch-tabular.readthedocs.io/en/latest/data/)** to define the target variable, as well as the names of the continuous and categorical variables.\n",
    "- Use **[TrainerConfig](https://pytorch-tabular.readthedocs.io/en/latest/training/)** to automatically tune the learning rate. Set batch_size to be 1024 and set max_epoch as 50.\n",
    "- Use **[CategoryEmbeddingModelConfig](https://pytorch-tabular.readthedocs.io/en/latest/models/#category-embedding-model)** to create a feedforward neural network with 1 hidden layer containing 50 neurons.\n",
    "- Use **[OptimizerConfig](https://pytorch-tabular.readthedocs.io/en/latest/optimizer/)** to choose Adam optimiser. There is no need to set the learning rate (since it will be tuned automatically) nor scheduler.\n",
    "- Use **[TabularModel](https://pytorch-tabular.readthedocs.io/en/latest/tabular_model/)** to initialise the model and put all the configs together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZZWAYdNhqPzh"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">03</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">14:20:33</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">141</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m03\u001b[0m-\u001b[1;36m11\u001b[0m \u001b[1;92m14:20:33\u001b[0m,\u001b[1;36m141\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Enter your code here\n",
    "data_config = DataConfig(\n",
    "    target=[\"resale_price\"],\n",
    "    continuous_cols=[\"dist_to_nearest_stn\", \"dist_to_dhoby\",\n",
    "\"degree_centrality\", \"eigenvector_centrality\", \"remaining_lease_years\",\n",
    "\"floor_area_sqm\"],\n",
    "    categorical_cols=[\"month\", \"town\", \"flat_model_type\", \"storey_range\"],\n",
    ")\n",
    "\n",
    "trainer_config = TrainerConfig(\n",
    "    batch_size=1024,\n",
    "    max_epochs=50,\n",
    ")\n",
    "\n",
    "model_config = CategoryEmbeddingModelConfig(\n",
    "    task=\"regression\",\n",
    "    layers=\"50\",  # Number of nodes in each layer\n",
    ")\n",
    "\n",
    "optimizer_config = OptimizerConfig() # Default is Adam\n",
    "\n",
    "tabular_model = TabularModel(\n",
    "    data_config=data_config,\n",
    "    trainer_config=trainer_config,\n",
    "    model_config=model_config,\n",
    "    optimizer_config=optimizer_config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2UXPKq0qWQG"
   },
   "source": [
    "> Report the test RMSE error and the test R2 value that you obtained.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zmE9Bc7Nqadi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">03</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">14:20:33</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">178</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m03\u001b[0m-\u001b[1;36m11\u001b[0m \u001b[1;92m14:20:33\u001b[0m,\u001b[1;36m178\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">03</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">14:20:33</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">227</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m03\u001b[0m-\u001b[1;36m11\u001b[0m \u001b[1;92m14:20:33\u001b[0m,\u001b[1;36m227\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">03</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">14:20:33</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">351</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: CategoryEmbeddingModel \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m03\u001b[0m-\u001b[1;36m11\u001b[0m \u001b[1;92m14:20:33\u001b[0m,\u001b[1;36m351\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: CategoryEmbeddingModel \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">03</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">14:20:33</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">392</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m03\u001b[0m-\u001b[1;36m11\u001b[0m \u001b[1;92m14:20:33\u001b[0m,\u001b[1;36m392\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">03</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">14:20:33</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">444</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m03\u001b[0m-\u001b[1;36m11\u001b[0m \u001b[1;92m14:20:33\u001b[0m,\u001b[1;36m444\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                      </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ _backbone        │ CategoryEmbeddingBackbone │  3.0 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ _embedding_layer │ Embedding1dLayer          │  1.6 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ head             │ LinearHead                │     51 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ loss             │ MSELoss                   │      0 │ train │\n",
       "└───┴──────────────────┴───────────────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                     \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ _backbone        │ CategoryEmbeddingBackbone │  3.0 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ _embedding_layer │ Embedding1dLayer          │  1.6 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ head             │ LinearHead                │     51 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ loss             │ MSELoss                   │      0 │ train │\n",
       "└───┴──────────────────┴───────────────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 4.6 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 4.6 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 16                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 4.6 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 4.6 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 16                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d2521ce2ee4aeb8484980469dcb4d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">03</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">14:22:35</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">975</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m03\u001b[0m-\u001b[1;36m11\u001b[0m \u001b[1;92m14:22:35\u001b[0m,\u001b[1;36m975\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">03</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">14:22:35</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">977</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m03\u001b[0m-\u001b[1;36m11\u001b[0m \u001b[1;92m14:22:35\u001b[0m,\u001b[1;36m977\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL omegaconf.dictconfig.DictConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([DictConfig])` or the `torch.serialization.safe_globals([DictConfig])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnpicklingError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# TODO: Enter your code here\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtabular_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m result = tabular_model.evaluate(test_data)\n\u001b[32m      4\u001b[39m pred_df = tabular_model.predict(test_data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alan Wong\\Desktop\\Assignment\\2025_NNDL_IndivAssignment\\.venv\\Lib\\site-packages\\pytorch_tabular\\tabular_model.py:806\u001b[39m, in \u001b[36mTabularModel.fit\u001b[39m\u001b[34m(self, train, validation, loss, metrics, metrics_prob_inputs, optimizer, optimizer_params, train_sampler, target_transform, max_epochs, min_epochs, seed, callbacks, datamodule, cache_data, handle_oom)\u001b[39m\n\u001b[32m    792\u001b[39m         warnings.warn(\n\u001b[32m    793\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtrain data and datamodule is provided.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    794\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m Ignoring the train data and using the datamodule.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    795\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m Set either one of them to None to avoid this warning.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    796\u001b[39m         )\n\u001b[32m    797\u001b[39m model = \u001b[38;5;28mself\u001b[39m.prepare_model(\n\u001b[32m    798\u001b[39m     datamodule,\n\u001b[32m    799\u001b[39m     loss,\n\u001b[32m   (...)\u001b[39m\u001b[32m    803\u001b[39m     optimizer_params \u001b[38;5;129;01mor\u001b[39;00m {},\n\u001b[32m    804\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandle_oom\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alan Wong\\Desktop\\Assignment\\2025_NNDL_IndivAssignment\\.venv\\Lib\\site-packages\\pytorch_tabular\\tabular_model.py:691\u001b[39m, in \u001b[36mTabularModel.train\u001b[39m\u001b[34m(self, model, datamodule, callbacks, max_epochs, min_epochs, handle_oom)\u001b[39m\n\u001b[32m    689\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mTraining the model completed\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    690\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.load_best:\n\u001b[32m--> \u001b[39m\u001b[32m691\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_best_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trainer\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alan Wong\\Desktop\\Assignment\\2025_NNDL_IndivAssignment\\.venv\\Lib\\site-packages\\pytorch_tabular\\tabular_model.py:1534\u001b[39m, in \u001b[36mTabularModel.load_best_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1532\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose:\n\u001b[32m   1533\u001b[39m         logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel Checkpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mckpt_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1534\u001b[39m     ckpt = \u001b[43mpl_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1535\u001b[39m     \u001b[38;5;28mself\u001b[39m.model.load_state_dict(ckpt[\u001b[33m\"\u001b[39m\u001b[33mstate_dict\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1536\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alan Wong\\Desktop\\Assignment\\2025_NNDL_IndivAssignment\\.venv\\Lib\\site-packages\\pytorch_tabular\\utils\\python_utils.py:85\u001b[39m, in \u001b[36mpl_load\u001b[39m\u001b[34m(path_or_url, map_location)\u001b[39m\n\u001b[32m     83\u001b[39m fs = get_filesystem(path_or_url)\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m fs.open(path_or_url, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alan Wong\\Desktop\\Assignment\\2025_NNDL_IndivAssignment\\.venv\\Lib\\site-packages\\torch\\serialization.py:1470\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1462\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1463\u001b[39m                     opened_zipfile,\n\u001b[32m   1464\u001b[39m                     map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1467\u001b[39m                     **pickle_load_args,\n\u001b[32m   1468\u001b[39m                 )\n\u001b[32m   1469\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1470\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1471\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1472\u001b[39m             opened_zipfile,\n\u001b[32m   1473\u001b[39m             map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1476\u001b[39m             **pickle_load_args,\n\u001b[32m   1477\u001b[39m         )\n\u001b[32m   1478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[31mUnpicklingError\u001b[39m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL omegaconf.dictconfig.DictConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([DictConfig])` or the `torch.serialization.safe_globals([DictConfig])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "# TODO: Enter your code here\n",
    "tabular_model.fit(train=train_data)\n",
    "result = tabular_model.evaluate(test_data)\n",
    "pred_df = tabular_model.predict(test_data)\n",
    "tabular_model.save_model(\"examples/basic\")\n",
    "loaded_model = TabularModel.load_model(\"examples/basic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore the error above, refer to https://github.com/pytorch/opacus/issues/690\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b90a41e248374aa99d29a8ce5cb1b624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       67060428800.0       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  test_mean_squared_error  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       67060428800.0       </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      67060428800.0      \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m test_mean_squared_error \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      67060428800.0      \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resale_price_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87370</th>\n",
       "      <td>221761.890625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87371</th>\n",
       "      <td>229285.078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87372</th>\n",
       "      <td>271750.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87373</th>\n",
       "      <td>272102.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87374</th>\n",
       "      <td>262278.906250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       resale_price_prediction\n",
       "87370            221761.890625\n",
       "87371            229285.078125\n",
       "87372            271750.125000\n",
       "87373            272102.906250\n",
       "87374            262278.906250"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = tabular_model.evaluate(test_data)\n",
    "pred_df = tabular_model.predict(test_data)\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 258960.28421362222\n",
      "R2 Score: -1.5351788847008763\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# pip install torcheval\n",
    "from torcheval.metrics.functional import r2_score\n",
    "# Extract predicted values\n",
    "predicted = torch.tensor(pred_df[\"resale_price_prediction\"].values)\n",
    "\n",
    "# Extract ground truth values\n",
    "actual = torch.tensor(test_data[\"resale_price\"].values)\n",
    "\n",
    "# Compute R2 score and RMSE\n",
    "r2 = r2_score(predicted, actual)\n",
    "rmse = math.sqrt(result[0]['test_mean_squared_error'])\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2 Score:\", r2.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEJhRU18qX22"
   },
   "source": [
    "> Print out the corresponding rows in the dataframe for the top 25 test samples with the largest errors. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "5ma5K9vKqZEq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        month  year             town              full_address   nearest_stn  \\\n",
      "106199     12  2021       QUEENSTOWN            92 DAWSON ROAD    Queenstown   \n",
      "90608      12  2021           BISHAN     273B BISHAN STREET 24        Bishan   \n",
      "100836      6  2021  KALLANG/WHAMPOA          39 JALAN BAHAGIA     Boon Keng   \n",
      "101237     11  2021  KALLANG/WHAMPOA          8 BOON KENG ROAD     Bendemeer   \n",
      "93930      12  2021     CENTRAL AREA        1D CANTONMENT ROAD   Outram Park   \n",
      "90483       9  2021           BISHAN     273A BISHAN STREET 24        Bishan   \n",
      "93931      12  2021     CENTRAL AREA        1B CANTONMENT ROAD   Outram Park   \n",
      "90432       8  2021           BISHAN     275A BISHAN STREET 24        Bishan   \n",
      "101087      9  2021  KALLANG/WHAMPOA          46 JALAN BAHAGIA     Boon Keng   \n",
      "112731      8  2021        TOA PAYOH  138A LORONG 1A TOA PAYOH      Braddell   \n",
      "93904      11  2021     CENTRAL AREA        1C CANTONMENT ROAD   Outram Park   \n",
      "93929      12  2021     CENTRAL AREA        1B CANTONMENT ROAD   Outram Park   \n",
      "112973     11  2021        TOA PAYOH  138B LORONG 1A TOA PAYOH     Toa Payoh   \n",
      "93670      12  2021      BUKIT TIMAH            6 TOH YI DRIVE  Beauty World   \n",
      "92443      11  2021      BUKIT MERAH        96A HENDERSON ROAD   Tiong Bahru   \n",
      "101236     11  2021  KALLANG/WHAMPOA          9 BOON KENG ROAD     Bendemeer   \n",
      "90523      10  2021           BISHAN     273B BISHAN STREET 24        Bishan   \n",
      "90253       4  2021           BISHAN     273B BISHAN STREET 24        Bishan   \n",
      "106132     11  2021       QUEENSTOWN     50 COMMONWEALTH DRIVE  Commonwealth   \n",
      "90431       8  2021           BISHAN     273A BISHAN STREET 24        Bishan   \n",
      "105702      6  2021       QUEENSTOWN       150 MEI LING STREET    Queenstown   \n",
      "92340      10  2021      BUKIT MERAH          56 HAVELOCK ROAD   Tiong Bahru   \n",
      "113043     12  2021        TOA PAYOH  139A LORONG 1A TOA PAYOH     Caldecott   \n",
      "93835       8  2021     CENTRAL AREA        1G CANTONMENT ROAD   Outram Park   \n",
      "90340       6  2021           BISHAN     273A BISHAN STREET 24        Bishan   \n",
      "\n",
      "        dist_to_nearest_stn  dist_to_dhoby  degree_centrality  \\\n",
      "106199             0.584731       3.882019           0.016807   \n",
      "90608              0.776182       6.297489           0.033613   \n",
      "100836             0.998313       3.304953           0.016807   \n",
      "101237             0.352251       2.587444           0.016807   \n",
      "93930              0.438348       2.506568           0.033613   \n",
      "90483              0.767244       6.327956           0.033613   \n",
      "93931              0.352779       2.413099           0.033613   \n",
      "90432              0.827889       6.370404           0.033613   \n",
      "101087             0.987682       3.383526           0.016807   \n",
      "112731             0.461414       4.151360           0.016807   \n",
      "93904              0.401367       2.445314           0.033613   \n",
      "93929              0.352779       2.413099           0.033613   \n",
      "112973             0.471864       4.101820           0.016807   \n",
      "93670              0.428356       8.948410           0.016807   \n",
      "92443              0.586629       2.932814           0.016807   \n",
      "101236             0.335875       2.535679           0.016807   \n",
      "90523              0.776182       6.297489           0.033613   \n",
      "90253              0.776182       6.297489           0.033613   \n",
      "106132             0.197249       5.421535           0.016807   \n",
      "90431              0.767244       6.327956           0.033613   \n",
      "105702             0.245207       4.709043           0.016807   \n",
      "92340              0.451387       2.128424           0.016807   \n",
      "113043             0.514517       4.107963           0.016807   \n",
      "93835              0.566594       2.661710           0.033613   \n",
      "90340              0.767244       6.327956           0.033613   \n",
      "\n",
      "        eigenvector_centrality                 flat_model_type  \\\n",
      "106199                0.008342  5 ROOM, Premium Apartment Loft   \n",
      "90608                 0.015854                    5 ROOM, DBSS   \n",
      "100836                0.053004                 3 ROOM, Terrace   \n",
      "101237                0.004414                    5 ROOM, DBSS   \n",
      "93930                 0.121082                 5 ROOM, Type S2   \n",
      "90483                 0.015854                    5 ROOM, DBSS   \n",
      "93931                 0.121082                 5 ROOM, Type S2   \n",
      "90432                 0.015854                    5 ROOM, DBSS   \n",
      "101087                0.053004                 3 ROOM, Terrace   \n",
      "112731                0.017995                    5 ROOM, DBSS   \n",
      "93904                 0.121082                 5 ROOM, Type S2   \n",
      "93929                 0.121082                 5 ROOM, Type S2   \n",
      "112973                0.036944                    5 ROOM, DBSS   \n",
      "93670                 0.001358           EXECUTIVE, Maisonette   \n",
      "92443                 0.047782                5 ROOM, Improved   \n",
      "101236                0.004414                    5 ROOM, DBSS   \n",
      "90523                 0.015854                    5 ROOM, DBSS   \n",
      "90253                 0.015854                    5 ROOM, DBSS   \n",
      "106132                0.005350                5 ROOM, Improved   \n",
      "90431                 0.015854                    5 ROOM, DBSS   \n",
      "105702                0.008342            EXECUTIVE, Apartment   \n",
      "92340                 0.047782                5 ROOM, Improved   \n",
      "113043                0.023913                    5 ROOM, DBSS   \n",
      "93835                 0.121082                 5 ROOM, Type S2   \n",
      "90340                 0.015854                    5 ROOM, DBSS   \n",
      "\n",
      "        remaining_lease_years  floor_area_sqm storey_range  resale_price  \\\n",
      "106199              93.333333           122.0     40 TO 42     1328000.0   \n",
      "90608               88.833333           120.0     37 TO 39     1360000.0   \n",
      "100836              50.083333           210.0     01 TO 03     1268000.0   \n",
      "101237              88.250000           119.0     40 TO 42     1268000.0   \n",
      "93930               88.166667           107.0     46 TO 48     1280000.0   \n",
      "90483               89.000000           120.0     37 TO 39     1295000.0   \n",
      "93931               88.083333           107.0     40 TO 42     1288000.0   \n",
      "90432               88.916667           120.0     25 TO 27     1280000.0   \n",
      "101087              49.833333           241.0     01 TO 03     1235000.0   \n",
      "112731              89.833333           114.0     40 TO 42     1238000.0   \n",
      "93904               88.333333           106.0     40 TO 42     1261000.0   \n",
      "93929               88.083333           106.0     43 TO 45     1254000.0   \n",
      "112973              89.500000           114.0     40 TO 42     1240000.0   \n",
      "93670               66.666667           154.0     04 TO 06     1238000.0   \n",
      "92443               96.583333           113.0     40 TO 42     1256000.0   \n",
      "101236              88.250000           119.0     34 TO 36     1230000.0   \n",
      "90523               88.916667           120.0     22 TO 24     1260000.0   \n",
      "90253               89.416667           120.0     31 TO 33     1250000.0   \n",
      "106132              92.333333           117.0     34 TO 36     1230000.0   \n",
      "90431               89.083333           120.0     22 TO 24     1240000.0   \n",
      "105702              73.416667           148.0     10 TO 12     1235000.0   \n",
      "92340               90.750000           114.0     34 TO 36     1245000.0   \n",
      "113043              89.416667           117.0     34 TO 36     1220000.0   \n",
      "93835               88.500000           107.0     46 TO 48     1210000.0   \n",
      "90340               89.250000           120.0     25 TO 27     1238800.0   \n",
      "\n",
      "        absolute_error  \n",
      "106199    1.082898e+06  \n",
      "90608     1.019613e+06  \n",
      "100836    9.793666e+05  \n",
      "101237    9.615951e+05  \n",
      "93930     9.561140e+05  \n",
      "90483     9.555068e+05  \n",
      "93931     9.478293e+05  \n",
      "90432     9.432002e+05  \n",
      "101087    9.384719e+05  \n",
      "112731    9.295776e+05  \n",
      "93904     9.273829e+05  \n",
      "93929     9.263332e+05  \n",
      "112973    9.244056e+05  \n",
      "93670     9.197348e+05  \n",
      "92443     9.195961e+05  \n",
      "101236    9.165911e+05  \n",
      "90523     9.160256e+05  \n",
      "90253     9.153364e+05  \n",
      "106132    9.055878e+05  \n",
      "90431     9.010418e+05  \n",
      "105702    9.006453e+05  \n",
      "92340     8.976992e+05  \n",
      "113043    8.959561e+05  \n",
      "93835     8.949868e+05  \n",
      "90340     8.945050e+05  \n"
     ]
    }
   ],
   "source": [
    "# TODO: Enter your code here\n",
    "predicted = pred_df[\"resale_price_prediction\"].values\n",
    "actual = test_data[\"resale_price\"].values\n",
    "\n",
    "# Calculate absolute errors\n",
    "absolute_errors = np.abs(predicted - actual)\n",
    "\n",
    "# Add the errors as a new column to the test_data DataFrame\n",
    "test_data_with_errors = test_data.copy()\n",
    "test_data_with_errors[\"absolute_error\"] = absolute_errors\n",
    "\n",
    "# Sort by absolute_error in descending order and get top 25 rows\n",
    "top_25_errors = test_data_with_errors.sort_values(by=\"absolute_error\", ascending=False).head(25)\n",
    "\n",
    "# Print the corresponding rows\n",
    "print(top_25_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part B, Q2 (10 marks)\n",
    "---\n",
    "In Question B1, we used the Category Embedding model. This creates a feedforward neural network in which the categorical features get learnable embeddings. In this question, we will make use of a library called Pytorch-WideDeep. This library makes it easy to work with multimodal deep-learning problems combining images, text, and tables. We will just be utilizing the deeptabular component of this library through the TabMlp network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch-widedeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_widedeep.preprocessing import TabPreprocessor\n",
    "from pytorch_widedeep.models import TabMlp, WideDeep\n",
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.metrics import R2Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Divide the dataset (‘hdb_price_prediction.csv’) into train and test sets by using entries from the year 2020 and before as training data, and entries from 2021 and after as the test data（validation set is not required here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87370, 14)\n",
      "(72183, 14)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Enter your code here\n",
    "df = pd.read_csv('hdb_price_prediction.csv')\n",
    "# Split into training and test sets\n",
    "train_data = df[df['year'] <= 2020]\n",
    "test_data = df[df['year'] >= 2021]\n",
    "# print(train_data.shape)\n",
    "# print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Refer to the documentation of Pytorch-WideDeep and perform the following tasks:\n",
    "https://pytorch-widedeep.readthedocs.io/en/latest/index.html\n",
    "* Use [**TabPreprocessor**](https://pytorch-widedeep.readthedocs.io/en/latest/examples/01_preprocessors_and_utils.html#2-tabpreprocessor) to create the deeptabular component using the continuous\n",
    "features and the categorical features. Use this component to transform the training dataset.\n",
    "* Create the [**TabMlp**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/model_components.html#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp) model with 2 hidden layers in the MLP, with 200 and 100 neurons respectively.\n",
    "* Create a [**Trainer**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/trainer.html#pytorch_widedeep.training.Trainer) for the training of the created TabMlp model with the root mean squared error (RMSE) cost function. Train the model for 60 epochs using this trainer, keeping a batch size of 64. (Note: set the *num_workers* parameter to 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('month', 12, 6), ('town', 26, 10), ('flat_model_type', 43, 13), ('storey_range', 17, 8)]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Enter your code here\n",
    "# Define categorical and continuous columns\n",
    "continuous_cols = [\"dist_to_nearest_stn\", \"dist_to_dhoby\",\n",
    "\"degree_centrality\", \"eigenvector_centrality\", \"remaining_lease_years\",\n",
    "\"floor_area_sqm\"]  # No need to specify embed_dim explicitly\n",
    "cat_embed_cols = [\"month\", \"town\", \"flat_model_type\", \"storey_range\"]\n",
    "\n",
    "# Initialize TabPreprocessor without specifying embed_dim\n",
    "tab_preprocessor = TabPreprocessor(\n",
    "    continuous_cols=continuous_cols,\n",
    "    cat_embed_cols=cat_embed_cols,\n",
    "    cols_to_scale = continuous_cols)\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_tab = tab_preprocessor.fit_transform(train_data)\n",
    "\n",
    "# Inspect the automatically computed embedding dimensions\n",
    "print(tab_preprocessor.cat_embed_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00,  1.00000000e+00,  1.00000000e+00, ...,\n",
       "        -1.45273689e-02, -1.03556571e+00, -2.21217732e+00],\n",
       "       [ 1.00000000e+00,  1.00000000e+00,  2.00000000e+00, ...,\n",
       "        -1.45273689e-02, -1.09350058e+00, -1.26511034e+00],\n",
       "       [ 1.00000000e+00,  1.00000000e+00,  2.00000000e+00, ...,\n",
       "        -1.96827730e-01, -9.51882003e-01, -1.26511034e+00],\n",
       "       ...,\n",
       "       [ 1.20000000e+01,  2.60000000e+01,  8.00000000e+00, ...,\n",
       "        -2.68613872e-01, -6.75082048e-01,  1.82315157e+00],\n",
       "       [ 1.20000000e+01,  2.60000000e+01,  2.00000000e+01, ...,\n",
       "        -2.68613872e-01, -6.62207631e-01,  1.98785887e+00],\n",
       "       [ 1.20000000e+01,  2.60000000e+01,  8.00000000e+00, ...,\n",
       "        -2.68613872e-01, -6.81519256e-01,  1.82315157e+00]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Report the test RMSE and the test R2 value that you obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part B, Q3 (10 marks)\n",
    "---\n",
    "Besides ensuring that your neural network performs well, it is important to be able to explain the model’s decision. **Captum** is a very handy library that helps you to do so for PyTorch models.\n",
    "\n",
    "Many model explainability algorithms for deep learning models are available in Captum. These algorithms are often used to generate an attribution score for each feature. Features with larger scores are more ‘important’ and some algorithms also provide information about directionality (i.e. a feature with very negative attribution scores means the larger the value of that feature, the lower the value of the output).\n",
    "\n",
    "In general, these algorithms can be grouped into two paradigms:\n",
    "- **perturbation based approaches** (e.g. Feature Ablation)\n",
    "- **gradient / backpropagation based approaches** (e.g. Saliency)\n",
    "\n",
    "The former adopts a brute-force approach of removing / permuting features one by one and does not scale up well. The latter depends on gradients and they can be computed relatively quickly. But unlike how backpropagation computes gradients with respect to weights, gradients here are computed **with respect to the input**. This gives us a sense of how much a change in the input affects the model’s outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import Saliency, InputXGradient, IntegratedGradients, GradientShap, FeatureAblation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> First, use the train set (year 2020 and before) and test set (year 2021) following the splits in Question B1 (validation set is not required here). To keep things simple, we will **limit our analysis to numeric / continuous features only**. Drop all categorical features from the dataframes. Standardise the features via **StandardScaler** (fit to training set, then transform all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Follow this tutorial to generate the plot from various model explainability algorithms (https://captum.ai/tutorials/House_Prices_Regression_Interpret).\n",
    "Specifically, make the following changes:\n",
    "- Use a feedforward neural network with 3 hidden layers, each having 5 neurons. Train using Adam optimiser with learning rate of 0.001.\n",
    "- Use Input x Gradients, Integrated Gradients, DeepLift, GradientSHAP, Feature Ablation. To avoid long running time, you can limit the analysis to the first 1000 samples in test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Read the following [descriptions](https://captum.ai/docs/attribution_algorithms) and [comparisons](https://captum.ai/docs/algorithms_comparison_matrix) in Captum to build up your understanding of the difference of various explainability algorithms. Based on your plot, identify the three most important features for regression. Explain how each of these features influences the regression outcome.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TODO: \\<Enter your answer here\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part B, Q4 (10 marks)\n",
    "---\n",
    "\n",
    "Model degradation is a common issue faced when deploying machine learning models (including neural networks) in the real world. New data points could exhibit a different pattern from older data points due to factors such as changes in government policy or market sentiments. For instance, housing prices in Singapore have been increasing and the Singapore government has introduced 3 rounds of cooling measures over the past years (16 December 2021, 30 September 2022, 27 April 2023).\n",
    "\n",
    "In such situations, the distribution of the new data points could differ from the original data distribution which the models were trained on. Recall that machine learning models often work with the assumption that the test distribution should be similar to train distribution. When this assumption is violated, model performance will be adversely impacted.  In the last part of this assignment, we will investigate to what extent model degradation has occurred.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install alibi-detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alibi_detect.cd import TabularDrift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Evaluate your model from B1 on data from year 2022 and report the test R2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Evaluate your model from B1 on data from year 2023 and report the test R2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Did model degradation occur for the deep learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TODO: \\<Enter your answer here\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model degradation could be caused by [various data distribution shifts](https://huyenchip.com/2022/02/07/data-distribution-shifts-and-monitoring.html#data-shift-types): covariate shift (features), label shift and/or concept drift (altered relationship between features and labels).\n",
    "There are various conflicting terminologies in the [literature](https://www.sciencedirect.com/science/article/pii/S0950705122002854#tbl1). Let’s stick to this reference for this assignment.\n",
    "\n",
    "> Using the **Alibi Detect** library, apply the **TabularDrift** function with the training data (year 2020 and before) used as the reference and **detect which features have drifted** in the 2023 test dataset. Before running the statistical tests, ensure you **sample 1000 data points** each from the train and test data. Do not use the whole train/test data. (Hint: use this example as a guide https://docs.seldon.io/projects/alibi-detect/en/stable/examples/cd_chi2ks_adult.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Assuming that the flurry of housing measures have made an impact on the relationship between all the features and resale_price (i.e. P(Y|X) changes), which type of data distribution shift possibly led to model degradation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TODO: \\<Enter your answer here\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From your analysis via TabularDrift, which features contribute to this shift?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TODO: \\<Enter your answer here\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Suggest 1 way to address model degradation and implement it, showing improved test R2 for year 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TODO: \\<Enter your answer here\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOG8ZhA98h3O6fnefkjOU9w",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
